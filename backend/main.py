import os
import requests
import sys
from bs4 import BeautifulSoup
from supabase import create_client, Client
from dotenv import load_dotenv

# Windowsç’°å¢ƒã®æ–‡å­—åŒ–ã‘å¯¾ç­–
if sys.platform == "win32":
    import codecs
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def fetch_html_from_url(url: str) -> str:
    """æŒ‡å®šã—ãŸURLã‹ã‚‰HTMLã‚’å–å¾—ã—ã€é©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨"""
    try:
        response = requests.get(url)
        response.raise_for_status()
        response.encoding = response.apparent_encoding  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º
        return response.text
    except requests.RequestException as e:
        print(f"âŒ URLã®å–å¾—ã«å¤±æ•—: {e}", file=sys.stdout, flush=True)
        return ""

def extract_title_and_content(html: str) -> tuple[str, str]:
    """HTMLã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ã‚ã‚Šï¼‰"""
    soup = BeautifulSoup(html, "html.parser")

    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    title = soup.title.string.strip() if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"

    # UTF-8 ã§ç¢ºå®Ÿã«å‡¦ç†
    title = title.encode('utf-8', errors='ignore').decode('utf-8')

    # æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆ<p>ã‚¿ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼‰
    paragraphs = soup.find_all("p")
    content = "\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
    
    content = content.encode('utf-8', errors='ignore').decode('utf-8')

    return title, content

def update_supabase_with_extracted_data(row_id: int, title: str, content: str):
    """Supabaseã®websitesãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°ï¼ˆUTF-8å¯¾å¿œï¼‰"""
    response = supabase.table("websites").update({
        "title": title,
        "content": content
    }).eq("id", row_id).execute()

    if response.data:
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜æˆåŠŸ (ID: {row_id})", file=sys.stdout, flush=True)
    else:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å¤±æ•— (ID: {row_id})", file=sys.stdout, flush=True)

def main():
    """æœªå‡¦ç†ã®URLã‚’å–å¾—ã—ã€ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’Supabaseã«ä¿å­˜"""
    response = supabase.table("websites").select("id, url").is_("title", None).execute()

    if response.data:
        for row in response.data:
            row_id = row["id"]
            url = row["url"]
            print(f"ğŸŒ URLã‚’å‡¦ç†ä¸­: {url}", file=sys.stdout, flush=True)

            # HTMLã‚’å–å¾—ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’æŠ½å‡º
            html = fetch_html_from_url(url)
            if html:
                title, content = extract_title_and_content(html)
                print(f"ğŸ¯ æŠ½å‡ºçµæœ: {title}", file=sys.stdout, flush=True)

                # Supabaseã®websitesãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                update_supabase_with_extracted_data(row_id, title, content)
    else:
        print("âœ”ï¸ ã™ã¹ã¦ã®URLãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚", file=sys.stdout, flush=True)

if __name__ == "__main__":
    main()
