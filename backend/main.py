import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from dotenv import load_dotenv

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def fetch_html_from_url(url: str) -> str:
    """æŒ‡å®šã—ãŸURLã‹ã‚‰HTMLã‚’å–å¾—ã™ã‚‹"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"âŒ URLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return ""

def extract_title_and_content(html: str) -> tuple[str, str]:
    """HTMLã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹"""
    soup = BeautifulSoup(html, "html.parser")

    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
    title = soup.title.string.strip() if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"

    # æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆ<p>ã‚¿ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼‰
    paragraphs = soup.find_all("p")
    content = "\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

    return title, content

def update_supabase_with_extracted_data(row_id: int, title: str, content: str):
    """Supabaseã®websitesãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°ã™ã‚‹"""
    # ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’çµ±ä¸€
    title = title.encode("utf-8").decode("utf-8")
    content = content.encode("utf-8").decode("utf-8")

    response = supabase.table("websites").update({
        "title": title,
        "content": content
    }).eq("id", row_id).execute()

    if response.error:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ (ID: {row_id}): {response.error}")
    else:
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸ (ID: {row_id})")

def main():
    # ã™ã§ã«å‡¦ç†æ¸ˆã¿ã§ãªã„URLã®ã¿å–å¾—ï¼ˆtitleãŒnullã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    response = supabase.table("websites").select("id, url").is_("title", None).execute()

    if response.error:
        print(f"âŒ Supabaseã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {response.error}")
        return

    if response.data:
        for row in response.data:
            row_id = row["id"]
            url = row["url"]
            print(f"ğŸ” URLã‚’å‡¦ç†ä¸­: {url}")

            # HTMLã‚’å–å¾—ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’æŠ½å‡º
            html = fetch_html_from_url(url)
            if html:
                title, content = extract_title_and_content(html)
                print(f"ğŸ“ æŠ½å‡ºçµæœ: ã‚¿ã‚¤ãƒˆãƒ«: {title}")

                # Supabaseã®websitesãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
                update_supabase_with_extracted_data(row_id, title, content)
    else:
        print("âœ… ã™ã¹ã¦ã®URLãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚")

if __name__ == "__main__":
    main()
